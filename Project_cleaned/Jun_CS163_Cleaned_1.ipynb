{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46db2e9b",
   "metadata": {},
   "source": [
    "# Required libararies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e20ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298d724",
   "metadata": {},
   "source": [
    "# Getting the Very first dataset on 7 Counties in SJV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9befbca0",
   "metadata": {},
   "source": [
    "# fetching SJV data on 4 pollutants \n",
    "\n",
    "- PM2.5\n",
    "- PM10\n",
    "- NO2\n",
    "- Ozone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c0f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 62201 for Fresno (019) in 1999...\n",
      "Success: 1249 records added.\n",
      "Fetching 62201 for Fresno (019) in 2000...\n",
      "Success: 1151 records added.\n",
      "Fetching 62201 for Fresno (019) in 2001...\n",
      "Success: 1319 records added.\n",
      "Fetching 62201 for Fresno (019) in 2002...\n",
      "Success: 1386 records added.\n",
      "Fetching 62201 for Fresno (019) in 2003...\n",
      "Success: 1635 records added.\n",
      "Fetching 62201 for Fresno (019) in 2004...\n",
      "Success: 1260 records added.\n",
      "Fetching 62201 for Fresno (019) in 2005...\n",
      "Success: 1090 records added.\n",
      "Fetching 62201 for Fresno (019) in 2006...\n",
      "Success: 1071 records added.\n",
      "Fetching 62201 for Fresno (019) in 2007...\n",
      "Success: 1095 records added.\n",
      "Fetching 62201 for Fresno (019) in 2008...\n",
      "Success: 1090 records added.\n",
      "Fetching 62201 for Fresno (019) in 2009...\n",
      "Success: 1032 records added.\n",
      "Fetching 62201 for Fresno (019) in 2010...\n",
      "Success: 1076 records added.\n",
      "Fetching 62201 for Fresno (019) in 2011...\n",
      "Success: 1091 records added.\n",
      "Fetching 62201 for Fresno (019) in 2012...\n",
      "Success: 1067 records added.\n",
      "Fetching 62201 for Fresno (019) in 2013...\n",
      "Success: 962 records added.\n",
      "Fetching 62201 for Fresno (019) in 2014...\n",
      "Success: 881 records added.\n",
      "Fetching 62201 for Fresno (019) in 2015...\n",
      "Success: 690 records added.\n",
      "Fetching 62201 for Fresno (019) in 2016...\n",
      "Success: 996 records added.\n",
      "Fetching 62201 for Fresno (019) in 2017...\n",
      "Success: 1088 records added.\n",
      "Fetching 62201 for Fresno (019) in 2018...\n",
      "Success: 1075 records added.\n",
      "Fetching 62201 for Fresno (019) in 2019...\n",
      "Success: 1093 records added.\n",
      "Fetching 62201 for Fresno (019) in 2020...\n",
      "Success: 1096 records added.\n",
      "Fetching 62201 for Fresno (019) in 2021...\n",
      "Success: 1091 records added.\n",
      "Fetching 62201 for Fresno (019) in 2022...\n",
      "Success: 1093 records added.\n",
      "Fetching 62201 for Fresno (019) in 2023...\n",
      "Success: 1095 records added.\n",
      "Final dataset contains 27772 records.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time  # To avoid hitting API rate limits\n",
    "\n",
    "# API endpoint for county-level data\n",
    "url = \"https://aqs.epa.gov/data/api/dailyData/byCounty\"\n",
    "\n",
    "# API Credentials\n",
    "email = \"dksxowns69@gmail.com\"\n",
    "api_key = \"aquabird37\"\n",
    "\n",
    "# Pollutant parameter codes\n",
    "params_list = [#\"61101\", # Wind Speed_Simple\n",
    "               #\"61103\", # Wind Speed_ Resultant (Vector-based)\n",
    "               #\"61104\", # Wind direction\n",
    "               #\"62101\", # Outdoor Temperature\n",
    "               #\"62201\", # Relative Humidity\n",
    "               #\"64101\", # Barometric Pressure\n",
    "               #\"42101\", # Carbon monoxide\n",
    "               #\"63101\", # Visibility\n",
    "               #\"63011\", # Solar Radiation\n",
    "               \"88101\", # PM2.5\n",
    "               \"85129\", # PM10\n",
    "               \"42602\", # NO2\n",
    "               \"44201\", # Ozone\n",
    "\n",
    "]\n",
    "\n",
    "# Year range\n",
    "years = range(1980, 2025)\n",
    "\n",
    "# California state code\n",
    "state_code = \"06\"\n",
    "\n",
    "# San Joaquin Valley county codes\n",
    "county_codes = {\n",
    "    \"San Joaquin\": \"077\",\n",
    "    \"Stanislaus\": \"099\",\n",
    "    \"Merced\": \"047\",\n",
    "    \"Fresno\": \"019\",\n",
    "    \"Kings\": \"031\",\n",
    "    \"Tulare\": \"107\",\n",
    "    \"Kern\": \"029\"\n",
    "}\n",
    "\n",
    "# Empty list to store data\n",
    "all_data = []\n",
    "\n",
    "# Loop through pollutants, counties, and years\n",
    "for param in params_list:\n",
    "    for county_name, county_code in county_codes.items():\n",
    "        for year in years:\n",
    "            print(f\"Fetching {param} for {county_name} ({county_code}) in {year}...\")\n",
    "\n",
    "            params = {\n",
    "                \"email\": email,\n",
    "                \"key\": api_key,\n",
    "                \"param\": param,\n",
    "                \"bdate\": f\"{year}0101\",\n",
    "                \"edate\": f\"{year}1231\",\n",
    "                \"state\": state_code,\n",
    "                \"county\": county_code\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if \"Data\" in data and data[\"Data\"]:\n",
    "                    all_data.extend(data[\"Data\"])\n",
    "                    print(f\"Success: {len(data['Data'])} records added.\")\n",
    "                else:\n",
    "                    print(f\"No data found for {param} in {county_name} ({year}).\")\n",
    "            else:\n",
    "                print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "            time.sleep(1)  # Avoid exceeding API rate limits\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save to CSV for future analysis\n",
    "df.to_csv(\"SJV_AQI_1980_2025.csv\", index=False)\n",
    "\n",
    "print(f\"Final dataset contains {df.shape[0]} records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5c124",
   "metadata": {},
   "source": [
    "# ML Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7143fa3a",
   "metadata": {},
   "source": [
    "## 1st SARIMA Attempt_Jun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARAMETERS ---\n",
    "ISR_YEAR = 2006  # Easy to update for Setting where Pre VS POST lies.\n",
    "forecast_start_date = \"2024-11-01\"\n",
    "pollutants = ['PM10 Total 0-10um STP', 'PM2.5 - Local Conditions', 'Ozone', 'Nitrogen dioxide (NO2)']\n",
    "counties = ['San Joaquin', 'Stanislaus', 'Merced', 'Fresno', 'Kings', 'Tulare', 'Kern']\n",
    "\n",
    "# --- LOAD AND CLEAN ---\n",
    "df = pd.read_csv(\"SJV_AQI_1980_2025.csv\")\n",
    "df = df[df['county'].isin(counties) & df['parameter'].isin(pollutants)]\n",
    "df = df[df['metric_used'] == 'Daily Mean']\n",
    "\n",
    "\n",
    "# Fix datetime and group\n",
    "df['datetime'] = pd.to_datetime(df['first_max_datetime'], errors='coerce')\n",
    "df = df.dropna(subset=['datetime'])\n",
    "df['date'] = df['datetime'].dt.date\n",
    "df['month'] = pd.to_datetime(df['datetime'].dt.to_period('M').astype(str))\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df = df.rename(columns={'parameter': 'pollutant', 'arithmetic_mean': 'value'})\n",
    "\n",
    "\n",
    "# --- FUNCTION: Forecast by time unit ---\n",
    "def forecast_by_timescale(grouped, freq, periods, label):\n",
    "    results = []\n",
    "    for (county, pollutant), group in grouped.groupby(['county', 'pollutant']):\n",
    "        ts = group.set_index('date').asfreq(freq)['value'].fillna(method='ffill')\n",
    "        if len(ts.dropna()) < 36:\n",
    "            continue\n",
    "        try:\n",
    "            model = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,12), enforce_stationarity=False, enforce_invertibility=False)\n",
    "            fit = model.fit(disp=False)\n",
    "            start = len(ts)\n",
    "            end = start + periods - 1\n",
    "            forecast_index = pd.date_range(ts.index[-1] + pd.tseries.frequencies.to_offset(freq), periods=periods, freq=freq)\n",
    "            forecast = fit.predict(start=start, end=end)\n",
    "            results.append(pd.DataFrame({\n",
    "                'date': forecast_index,\n",
    "                'predicted_value': forecast.values,\n",
    "                'county': county,\n",
    "                'pollutant': pollutant,\n",
    "                'scale': label\n",
    "            }))\n",
    "        except:\n",
    "            continue\n",
    "    return pd.concat(results)\n",
    "\n",
    "\n",
    "\n",
    "# --- FORECAST EXECUTION ---\n",
    "daily = df.groupby(['county', 'pollutant', 'datetime'])['value'].mean().reset_index()\n",
    "daily = daily.rename(columns={'datetime': 'date'})\n",
    "monthly = df.groupby(['county', 'pollutant', 'month'])['value'].mean().reset_index()\n",
    "monthly = monthly.rename(columns={'month': 'date'})\n",
    "yearly = df.groupby(['county', 'pollutant', 'year'])['value'].mean().reset_index()\n",
    "yearly['date'] = pd.to_datetime(yearly['year'].astype(str) + \"-01-01\")\n",
    "\n",
    "daily_forecast = forecast_by_timescale(daily, 'D', 10, 'daily')\n",
    "monthly_forecast = forecast_by_timescale(monthly, 'MS', 10, 'monthly')\n",
    "yearly_forecast = forecast_by_timescale(yearly, 'YS', 10, 'yearly')\n",
    "\n",
    "forecast_df = pd.concat([daily_forecast, monthly_forecast, yearly_forecast])\n",
    "forecast_df.to_csv(\"SJV_AQI_Predictions_AllScales.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- PLOT: Per pollutant per timescale ---\n",
    "for scale in ['daily', 'monthly', 'yearly']:\n",
    "    scale_df = forecast_df[forecast_df['scale'] == scale]\n",
    "    for pollutant in scale_df['pollutant'].unique():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.lineplot(data=scale_df[scale_df['pollutant'] == pollutant], x='date', y='predicted_value', hue='county', marker='o')\n",
    "        plt.title(f\"Forecast for {pollutant} ({scale.capitalize()})\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel(\"Predicted Value (µg/m³)\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- PLOT: Combined plot per scale, averaged across counties ---\n",
    "for scale in ['daily', 'monthly', 'yearly']:\n",
    "    combined = (\n",
    "        forecast_df[forecast_df['scale'] == scale]\n",
    "        .groupby(['date', 'pollutant'])['predicted_value']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=combined, x='date', y='predicted_value', hue='pollutant', marker='o')\n",
    "    plt.title(f\"Average Forecast per Pollutant ({scale.capitalize()})\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"Average Predicted Value (µg/m³)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f628d0",
   "metadata": {},
   "source": [
    "## 2nd SARIMA Attempt_Jun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "merged_df = pd.read_csv(\"ready_pm25_fresno_with_Date.csv\")\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "merged_df = merged_df.sort_values('date')\n",
    "merged_df.set_index('date', inplace=True)\n",
    "\n",
    "# Target series\n",
    "series = merged_df['aqi_smoothed'].dropna()\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(series) * 0.8)\n",
    "train, test = series.iloc[:split], series.iloc[split:]\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train/test are defined\n",
    "model = SARIMAX(train,\n",
    "                order=(1,1,1),\n",
    "                seasonal_order=(1,1,1,7),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "# Forecast\n",
    "forecast = results.forecast(steps=len(test))\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(test, forecast)\n",
    "rmse = np.sqrt(mean_squared_error(test, forecast))\n",
    "r2 = r2_score(test, forecast)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(train.index, train, label='Train')\n",
    "plt.plot(test.index, test, label='Test', color='orange')\n",
    "plt.plot(test.index, forecast, label='Forecast', color='green')\n",
    "plt.title('SARIMA Forecast vs Actual AQI')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('AQI (Smoothed)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1724f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "662c5258",
   "metadata": {},
   "source": [
    "# Statistical Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41062fef",
   "metadata": {},
   "source": [
    "## T-Test: Pre vs Post ISR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- T-TEST: Pre vs Post ISR ---\n",
    "ttest_results = []\n",
    "\n",
    "for county in counties:\n",
    "    for pollutant in pollutants:\n",
    "        data = df[(df['county'] == county) & (df['pollutant'] == pollutant)]\n",
    "        pre = data[data['year'] < ISR_YEAR]['value'].dropna()\n",
    "        post = data[data['year'] >= ISR_YEAR]['value'].dropna()\n",
    "        if len(pre) > 10 and len(post) > 10:\n",
    "            t_stat, p_val = ttest_ind(pre, post, equal_var=False)\n",
    "            ttest_results.append({\n",
    "                'county': county,\n",
    "                'pollutant': pollutant,\n",
    "                'pre_mean': pre.mean(),\n",
    "                'post_mean': post.mean(),\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_val,\n",
    "                'significant': p_val < 0.05\n",
    "            })\n",
    "\n",
    "ttest_df = pd.DataFrame(ttest_results)\n",
    "\n",
    "# Save results to a CSV file\n",
    "ttest_df.to_csv(\"ISR_TTest_Results.csv\", index=False)\n",
    "\n",
    "# Print all results (not just significant ones)\n",
    "print(\"T-test completed. All results:\\n\")\n",
    "print(ttest_df)  # This will print all results, including non-significant ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8c92d",
   "metadata": {},
   "source": [
    " Conclusion: There has been a statistically significant improvement in air quality after 2006.\n",
    "\n",
    "Why?\n",
    "\n",
    "Every row shows significant = True, meaning the p-values are all below 0.05 (many even far below 0.001).\n",
    "\n",
    "For each county and pollutant:\n",
    "\n",
    "The post_mean is consistently lower than the pre_mean, indicating improved air quality.\n",
    "\n",
    "The t-statistics are large and positive, which aligns with this drop being statistically meaningful.\n",
    "\n",
    "📌 What does this mean in plain terms?\n",
    "For nearly every pollutant and county:\n",
    "\n",
    "PM10 and PM2.5 levels dropped significantly after 2006, the assumed year of the ISR (Incentive-based Smog Reduction, or similar air regulation).\n",
    "\n",
    "This suggests that the ISR policy had a real, measurable positive effect on reducing harmful air pollutants in the San Joaquin Valley.\n",
    "\n",
    "📉 Example Breakdown:\n",
    "\n",
    "County\tPollutant                        Pre-ISR Avg\t  Post-ISR Avg\t  Change\n",
    "Merced\tPM2.5 - Local Conditions\t         20.86\t        13.19\t       ↓ 36.8%\n",
    "Fresno\tPM2.5 - Local Conditions\t         20.05\t        15.05\t       ↓ 25.0%\n",
    "Tulare\tPM10 Total 0-10um STP            \t 51.14\t        44.96\t       ↓ 12.1%\n",
    "\n",
    "All of these are substantial reductions.\n",
    "\n",
    "🧪 T-test Interpretation Summary:\n",
    "Metric\tMeaning\n",
    "t_stat\tMagnitude of difference relative to variance. Larger = stronger effect.\n",
    "p_value\tProbability result is due to chance. < 0.05 = statistically significant.\n",
    "significant\tBoolean indicating if difference is statistically meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e4b14",
   "metadata": {},
   "source": [
    "### T-Test result Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00089e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the ttest_df to only include PM2.5 rows\n",
    "df_pm25_ttest = ttest_df[ttest_df['pollutant'] == 'PM2.5 - Local Conditions']\n",
    "\n",
    "# Calculate the percentage change and direction for the table\n",
    "df_pm25_ttest['Change'] = ((df_pm25_ttest['post_mean'] - df_pm25_ttest['pre_mean']) / df_pm25_ttest['pre_mean']) * 100\n",
    "df_pm25_ttest['Change'] = df_pm25_ttest['Change'].round(1).astype(str) + '%'\n",
    "\n",
    "# Round Pre-ISR and Post-ISR averages to 2 decimal places\n",
    "df_pm25_ttest[['pre_mean', 'post_mean']] = df_pm25_ttest[['pre_mean', 'post_mean']].round(2)\n",
    "\n",
    "# Add direction symbols based on the change value\n",
    "df_pm25_ttest['Direction'] = df_pm25_ttest['Change'].apply(lambda x: '↓' if '-' in x else '↑')\n",
    "\n",
    "# Create the final table with selected columns\n",
    "df_pm25_ttest_table = df_pm25_ttest[['county', 'pollutant', 'pre_mean', 'post_mean', 'Change', 'Direction']]\n",
    "\n",
    "# Display the table\n",
    "df_pm25_ttest_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524314f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the ttest_df to only include PM2.5 rows\n",
    "df_pm25_ttest = ttest_df[ttest_df['pollutant'] == 'PM10 Total 0-10um STP']\n",
    "\n",
    "# Calculate the percentage change and direction for the table\n",
    "df_pm25_ttest['Change'] = ((df_pm25_ttest['post_mean'] - df_pm25_ttest['pre_mean']) / df_pm25_ttest['pre_mean']) * 100\n",
    "df_pm25_ttest['Change'] = df_pm25_ttest['Change'].round(1).astype(str) + '%'\n",
    "\n",
    "# Round Pre-ISR and Post-ISR averages to 2 decimal places\n",
    "df_pm25_ttest[['pre_mean', 'post_mean']] = df_pm25_ttest[['pre_mean', 'post_mean']].round(2)\n",
    "\n",
    "# Add direction symbols based on the change value\n",
    "df_pm25_ttest['Direction'] = df_pm25_ttest['Change'].apply(lambda x: '↓' if '-' in x else '↑')\n",
    "\n",
    "# Create the final table with selected columns\n",
    "df_pm25_ttest_table = df_pm25_ttest[['county', 'pollutant', 'pre_mean', 'post_mean', 'Change', 'Direction']]\n",
    "\n",
    "# Display the table\n",
    "df_pm25_ttest_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Bar Plot for T-test Results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with subplots for each pollutant\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Barplot for PM10\n",
    "pm10_data = ttest_df[ttest_df['pollutant'].str.contains(\"PM10\")]\n",
    "axes[0].bar(pm10_data['county'], pm10_data['pre_mean'], width=0.4, label='Pre-ISR', align='center', color='darkgreen')\n",
    "axes[0].bar(pm10_data['county'], pm10_data['post_mean'], width=0.4, label='Post-ISR', align='edge', color='lightgreen')\n",
    "axes[0].set_title('PM10 - Pre vs Post ISR')\n",
    "axes[0].set_xlabel('County')\n",
    "axes[0].set_ylabel('AQI')\n",
    "axes[0].legend()\n",
    "\n",
    "# Barplot for PM2.5\n",
    "pm25_data = ttest_df[ttest_df['pollutant'].str.contains(\"PM2.5\")]\n",
    "axes[1].bar(pm25_data['county'], pm25_data['pre_mean'], width=0.4, label='Pre-ISR', align='center', color='darkgreen')\n",
    "axes[1].bar(pm25_data['county'], pm25_data['post_mean'], width=0.4, label='Post-ISR', align='edge', color='lightgreen')\n",
    "axes[1].set_title('PM2.5 - Pre vs Post ISR')\n",
    "axes[1].set_xlabel('County')\n",
    "axes[1].set_ylabel('AQI')\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93beba4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
